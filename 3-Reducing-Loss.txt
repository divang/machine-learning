- Reducing Loss: Gradient Descent
  - Convex problems 
    - It have only one minimum; that is, only one place where the slope is exactly 0. That minimum is where the loss function converges.
  - Gradient Descent
    - Calculating the loss function for every conceivable value of  over the entire data set would be an inefficient way of finding the convergence point. Let's examine a better mechanism—very popular in machine learning—called gradient descent.  
    a gradient is a vector, so it has both of the following characteristics:
      - a direction
      - a magnitude
    - Learning rate (also sometimes called step size)
      - the gradient vector has both a direction and a magnitude. Gradient descent algorithms multiply the gradient by a scalar known as the learning rate (also sometimes called step size) to determine the next point.     
    - Hyperparameters 
      - These are the knobs that programmers tweak in machine learning algorithms. Most machine learning programmers spend a fair amount of time tuning the learning rate. If you pick a learning rate that is too small, learning will take too long:
    - batch
      -  total number of examples you use to calculate the gradient in a single iteration. So far, we've assumed that the batch has been the entire data set. When working at Google scale, data sets often contain billions or even hundreds of billions of examples. Furthermore, Google data sets often contain huge numbers of features. Consequently, a batch can be enormous. A very large batch may cause even a single iteration to take a very long time to compute.
    - Stochastic gradient descent (SGD) 
      - takes this idea to the extreme--it uses only a single example (a batch size of 1) per iteration. Given enough iterations, SGD works but is very noisy. The term "stochastic" indicates that the one example comprising each batch is chosen at random.  
    - Mini-batch stochastic gradient descent (mini-batch SGD) 
      - is a compromise between full-batch iteration and SGD. A mini-batch is typically between 10 and 1,000 examples, chosen at random. Mini-batch SGD reduces the amount of noise in SGD but is still more efficient than full-batch.  
