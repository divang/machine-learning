- Regression analysis:
  - a set of statistical processes for estimating the relationships between a dependent variable 
    (often called the 'outcome variable') and one or more independent variables 
    (often called 'predictors', 'covariates', or 'features').

- Linear Regression:
  - linear approach to modeling the relationship between a scalar response 
    (or dependent variable) and one or more explanatory variables (or independent variables).
    y = ax + b
    
- Training a Model
  - It means learning (determining) good values for all the weights and the bias from labeled examples. In supervised learning, a machine learning algorithm builds a model by examining many examples and attempting to find a model that minimizes loss; this process is called "empirical risk minimization".
  
- Loss 
  - It is the penalty for a bad prediction. That is, loss is a number indicating how bad the model's prediction was on a single example. If the model's prediction is perfect, the loss is zero; otherwise, the loss is greater. The goal of training a model is to find a set of weights and biases that have low loss, on average, across all examples.  
  
- Squared loss: a popular loss function (also known as L2 loss)
  - The linear regression models we'll examine here use a loss function
    = the square of the difference between the label and the prediction
    = (observation - prediction(x)) <Square>
    = (y - y') power 2
  - Mean square error (MSE) is the average squared loss per example over the whole dataset. To calculate MSE, sum up all the squared losses for individual examples and then divide by the number of examples:
    - https://developers.google.com/machine-learning/crash-course/descending-into-ml/training-and-loss#squared-loss:-a-popular-loss-function
  
    
